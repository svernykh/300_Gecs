{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_torch_cuda():\n",
    "    try:\n",
    "        import pip\n",
    "        print(\"Installing PyTorch with CUDA support...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                             \"torch\", \"torchvision\", \"torchaudio\", \n",
    "                             \"--index-url\", \"https://download.pytorch.org/whl/cu121\"])\n",
    "        print(\"Installation completed. Please restart the kernel.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during installation: {str(e)}\")\n",
    "        \n",
    "install_torch_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03922431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "\n",
      "Checking for NVIDIA GPU...\n",
      "NVIDIA GPU detected:\n",
      "Mon Sep 22 16:38:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.88                 Driver Version: 576.88         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   60C    P0             24W /  120W |    1823MiB /   6141MiB |     16%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2052    C+G   ...ram Files\\Zen Browser\\zen.exe      N/A      |\n",
      "|    0   N/A  N/A            2376    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "|    0   N/A  N/A            5456    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A            5740    C+G   ...ram Files\\Zen Browser\\zen.exe      N/A      |\n",
      "|    0   N/A  N/A            8960    C+G   ...lpaper_engine\\wallpaper32.exe      N/A      |\n",
      "|    0   N/A  N/A           10080    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           10420    C+G   ...2txyewy\\CrossDeviceResume.exe      N/A      |\n",
      "|    0   N/A  N/A           12584    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           12608    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           14708    C+G   ....0.3485.81\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           15772    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           18360    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           18892    C+G   ...yb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A           18928    C+G   ...__8wekyb3d8bbwe\\XboxPcApp.exe      N/A      |\n",
      "|    0   N/A  N/A           19076    C+G   ...8aw4\\AcerPurifiedVoiceApp.exe      N/A      |\n",
      "|    0   N/A  N/A           19148    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           19932    C+G   ...7efx6bt\\app\\PredatorSense.exe      N/A      |\n",
      "|    0   N/A  N/A           20820    C+G   ...0_x64__8j3eq9eme6ctt\\IGCC.exe      N/A      |\n",
      "|    0   N/A  N/A           21044    C+G   ...cord\\app-1.0.9209\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A           21132    C+G   ..._8wekyb3d8bbwe\\XboxPcTray.exe      N/A      |\n",
      "|    0   N/A  N/A           21328    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           22000    C+G   ...ntrolPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A           23752    C+G   ...x64__dt26b99r8h8gj\\RtkUWP.exe      N/A      |\n",
      "|    0   N/A  N/A           25320    C+G   ...a\\Roaming\\Spotify\\Spotify.exe      N/A      |\n",
      "|    0   N/A  N/A           26400    C+G   D:\\Microsoft VS Code\\Code.exe         N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def check_gpu():\n",
    "    try:\n",
    "        # Check if NVIDIA GPU is present\n",
    "        nvidia_smi = subprocess.check_output(\"nvidia-smi\", shell=True)\n",
    "        print(\"NVIDIA GPU detected:\")\n",
    "        print(nvidia_smi.decode())\n",
    "        return True\n",
    "    except:\n",
    "        print(\"No NVIDIA GPU detected or nvidia-smi not found\")\n",
    "        return False\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"\\nChecking for NVIDIA GPU...\")\n",
    "has_gpu = check_gpu()\n",
    "\n",
    "if not has_gpu:\n",
    "    print(\"\\nTo use CUDA, you need:\")\n",
    "    print(\"1. An NVIDIA GPU\")\n",
    "    print(\"2. NVIDIA GPU drivers installed\")\n",
    "    print(\"3. CUDA Toolkit installed\")\n",
    "    print(\"\\nPlease install these if not already present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87cea9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486de100",
   "metadata": {},
   "source": [
    "# Klasifikasi Gambar Makanan Indonesia dengan EfficientNet-B3\n",
    "\n",
    "Notebook ini menggabungkan fungsionalitas pembacaan data dari `datareader.py` dengan kode pelatihan dari `train_3.py`. Kita akan menggunakan EfficientNet-B3 dengan transfer learning untuk mengklasifikasikan gambar makanan Indonesia.\n",
    "\n",
    "## Daftar isi\n",
    "1. Persiapan dan pembacaan data\n",
    "2. Penyiapan model dengan transfer learning\n",
    "3. Pelatihan dengan pelacakan progres\n",
    "4. Visualisasi dan evaluasi hasil\n",
    "\n",
    "## Latar Belakang\n",
    "EfficientNet-B3 adalah arsitektur jaringan saraf yang dioptimalkan untuk klasifikasi gambar. 300 Gecs menggunakan transfer learning dengan bobot pre-trained dari ImageNet untuk memanfaatkan fitur-fitur yang telah dipelajari sebelumnya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4621e",
   "metadata": {},
   "source": [
    "## 1. Import Library yang Dibutuhkan\n",
    "\n",
    "Pertama, kita akan mengimpor semua library yang diperlukan dan menyiapkan lingkungan. Kita menggunakan:\n",
    "- PyTorch: Framework deep learning utama\n",
    "- torchvision: Untuk model EfficientNet dan transformasi gambar\n",
    "- OpenCV (cv2): Untuk pemrosesan gambar\n",
    "- tqdm: Untuk pelacakan progres\n",
    "- scikit-learn: Untuk metrik evaluasi\n",
    "- matplotlib & seaborn: Untuk visualisasi\n",
    "- Library pendukung lainnya\n",
    "\n",
    "Kita juga akan mengatur seed acak untuk memastikan hasil yang dapat direproduksi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8abc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 2025\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Utility function for device setup (previously in utils.py)\n",
    "def check_set_gpu(override=None):\n",
    "    if override == None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "            print(f\"Using MPS: {torch.backends.mps.is_available()}\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print(f\"Using CPU: {torch.device('cpu')}\")\n",
    "    else:\n",
    "        device = torch.device(override)\n",
    "    return device\n",
    "\n",
    "# Set device\n",
    "device = check_set_gpu()  # Using the integrated utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b16a09",
   "metadata": {},
   "source": [
    "## 2. Implementasi Dataset\n",
    "\n",
    "Kita akan mengimplementasikan kelas `MakananIndo` yang merupakan turunan dari `torch.utils.data.Dataset`. Kelas ini akan:\n",
    "\n",
    "- Memuat gambar dari direktori yang ditentukan\n",
    "- Menangani pembagian data training dan validasi\n",
    "- Menerapkan transformasi gambar yang diperlukan\n",
    "- Normalisasi menggunakan statistik ImageNet\n",
    "- Mengembalikan pasangan gambar-label untuk pelatihan\n",
    "\n",
    "### Fitur Penting:\n",
    "1. **Pembagian Data**: 80% training, 20% validasi\n",
    "2. **Transformasi Default**: \n",
    "   - Konversi ke tensor PyTorch\n",
    "   - Normalisasi menggunakan mean dan std ImageNet\n",
    "3. **Format Gambar**: \n",
    "   - Ukuran diseragamkan ke 300x300 pixel\n",
    "   - Konversi warna dari BGR ke RGB\n",
    "   - Normalisasi nilai pixel ke rentang [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cdffd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakananIndo(Dataset):\n",
    "    # ImageNet normalization values\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_dir='IF25-4041-dataset/train',\n",
    "                 img_size=(300, 300),\n",
    "                 transform=None,\n",
    "                 split='train'\n",
    "                 ):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        # List all image files\n",
    "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        self.image_files.sort()\n",
    "        \n",
    "        # Load labels from CSV\n",
    "        csv_path = os.path.join(os.path.dirname(data_dir), 'train.csv')\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.label_dict = dict(zip(df['filename'], df['label']))\n",
    "        self.labels = [self.label_dict.get(f, None) for f in self.image_files]\n",
    "        \n",
    "        # Create train/val split\n",
    "        all_data = list(zip(self.image_files, self.labels))\n",
    "        total_len = len(all_data)\n",
    "        train_len = int(0.8 * total_len)\n",
    "        \n",
    "        indices = list(range(total_len))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.data = [all_data[i] for i in indices[:train_len]]\n",
    "        elif split == 'val':\n",
    "            self.data = [all_data[i] for i in indices[train_len:]]\n",
    "        else:\n",
    "            raise ValueError(\"Split must be 'train' or 'val'\")\n",
    "        \n",
    "        # Define default transforms\n",
    "        self.default_transform = Compose([\n",
    "            ToTensor(),\n",
    "            Normalize(mean=self.IMAGENET_MEAN, std=self.IMAGENET_STD)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.data_dir, self.data[idx][0])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, self.img_size)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = self.default_transform(image)\n",
    "        \n",
    "        label = self.data[idx][1]\n",
    "        \n",
    "        return image, label, img_path\n",
    "\n",
    "# Create helper function for label encoding\n",
    "def create_label_encoder(dataset):\n",
    "    \"\"\"Create a mapping from string labels to numeric indices\"\"\"\n",
    "    all_labels = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label, _ = dataset[i]\n",
    "        all_labels.append(label)\n",
    "    \n",
    "    unique_labels = sorted(list(set(all_labels)))\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    idx_to_label = {idx: label for idx, label in enumerate(unique_labels)}\n",
    "    \n",
    "    return label_to_idx, idx_to_label, unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494fe8e3",
   "metadata": {},
   "source": [
    "## 3. Fungsi-fungsi Pelatihan\n",
    "\n",
    "\n",
    "\n",
    "### Fungsi train_one_epoch:\n",
    "- Melatih model untuk satu epoch\n",
    "- Menghitung loss dan akurasi training\n",
    "- Melakukan optimisasi parameter model\n",
    "- Menampilkan progress bar dengan metrik real-time\n",
    "\n",
    "### Fungsi validate:\n",
    "- Mengevaluasi model pada data validasi\n",
    "- Menghitung loss dan akurasi validasi\n",
    "- Tidak melakukan backpropagation\n",
    "- Menampilkan progress bar dengan metrik\n",
    "\n",
    "### Fitur Umum:\n",
    "- Konversi label string ke indeks numerik\n",
    "- Penggunaan tqdm untuk visualisasi progres\n",
    "- Penanganan batch processing yang efisien\n",
    "- Perhitungan metrik secara real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2041dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, label_to_idx):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for batch_idx, (inputs, labels_tuple, _) in enumerate(pbar):\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Convert string labels to numeric indices\n",
    "        if isinstance(labels_tuple, (tuple, list)):\n",
    "            if isinstance(labels_tuple[0], str):\n",
    "                label_indices = [label_to_idx[label] for label in labels_tuple]\n",
    "            else:\n",
    "                label_indices = labels_tuple\n",
    "            targets = torch.tensor(label_indices, dtype=torch.long).to(device)\n",
    "        else:\n",
    "            targets = torch.tensor(labels_tuple, dtype=torch.long).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': total_loss/(batch_idx+1),\n",
    "            'acc': 100.*correct/total\n",
    "        })\n",
    "    \n",
    "    return total_loss/len(dataloader), 100.*correct/total\n",
    "\n",
    "def validate(model, dataloader, criterion, device, label_to_idx):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for batch_idx, (inputs, labels_tuple, _) in enumerate(pbar):\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Convert string labels to numeric indices\n",
    "            if isinstance(labels_tuple, (tuple, list)):\n",
    "                if isinstance(labels_tuple[0], str):\n",
    "                    label_indices = [label_to_idx[label] for label in labels_tuple]\n",
    "                else:\n",
    "                    label_indices = labels_tuple\n",
    "                targets = torch.tensor(label_indices, dtype=torch.long).to(device)\n",
    "            else:\n",
    "                targets = torch.tensor(labels_tuple, dtype=torch.long).to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': total_loss/(batch_idx+1),\n",
    "                'acc': 100.*correct/total\n",
    "            })\n",
    "    \n",
    "    return total_loss/len(dataloader), 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf5f15c",
   "metadata": {},
   "source": [
    "## 4. Persiapan Dataset dan DataLoader\n",
    "\n",
    "\n",
    "\n",
    "1. **Menyiapkan Hyperparameter**:\n",
    "   - Jumlah epoch: 30\n",
    "   - Ukuran batch: 32\n",
    "   - Learning rate: 0.01\n",
    "   - Ukuran gambar: 300x300\n",
    "\n",
    "2. **Membuat Dataset**:\n",
    "   - Dataset training dengan augmentasi\n",
    "   - Dataset validasi untuk evaluasi\n",
    "\n",
    "3. **Pengkodean Label**:\n",
    "   - Konversi label string ke indeks numerik\n",
    "   - Membuat mapping dua arah (indeks ↔ label)\n",
    "\n",
    "4. **Konfigurasi DataLoader**:\n",
    "   - Mengatur jumlah worker untuk loading parallel\n",
    "   - Mengaktifkan shuffling untuk data training\n",
    "   - Mengoptimalkan memory dengan pin_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822130ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Train dataset size: 866\n",
      "Validation dataset size: 217\n",
      "\n",
      "Creating label encoder...\n",
      "Number of classes: 5\n",
      "Classes: ['bakso', 'gado_gado', 'nasi_goreng', 'rendang', 'soto_ayam']\n",
      "Label to index mapping: {'bakso': 0, 'gado_gado': 1, 'nasi_goreng': 2, 'rendang': 3, 'soto_ayam': 4}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 32\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "img_size = (300, 300)  # EfficientNet-B3 input size\n",
    "\n",
    "# Create datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = MakananIndo(\n",
    "    data_dir='IF25-4041-dataset/train',\n",
    "    img_size=img_size,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "val_dataset = MakananIndo(\n",
    "    data_dir='IF25-4041-dataset/train',\n",
    "    img_size=img_size,\n",
    "    split='val'\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Create label encoder\n",
    "print(\"\\nCreating label encoder...\")\n",
    "label_to_idx, idx_to_label, unique_labels = create_label_encoder(train_dataset)\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {unique_labels}\")\n",
    "print(f\"Label to index mapping: {label_to_idx}\")\n",
    "\n",
    "# Create data loaders\n",
    "cpu_count = os.cpu_count()\n",
    "nworkers = cpu_count - 4 if cpu_count > 4 else 2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=nworkers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=nworkers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb531181",
   "metadata": {},
   "source": [
    "## Debug Pemuatan Dataset\n",
    "\n",
    "Mari kita uji pemuatan dataset dengan satu worker untuk mengisolasi masalah potensial:\n",
    "- Memuat beberapa sampel dari dataset\n",
    "- Memeriksa bentuk tensor dan label\n",
    "- Memastikan path gambar valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b61520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset loading...\n",
      "Successfully loaded image 1:\n",
      "- Shape: torch.Size([3, 300, 300])\n",
      "- Label: gado_gado\n",
      "- Path: IF25-4041-dataset/train\\0088.jpg\n",
      "--------------------------------------------------\n",
      "Successfully loaded image 2:\n",
      "- Shape: torch.Size([3, 300, 300])\n",
      "- Label: bakso\n",
      "- Path: IF25-4041-dataset/train\\0203.jpg\n",
      "--------------------------------------------------\n",
      "Successfully loaded image 3:\n",
      "- Shape: torch.Size([3, 300, 300])\n",
      "- Label: bakso\n",
      "- Path: IF25-4041-dataset/train\\0629.jpg\n",
      "--------------------------------------------------\n",
      "Successfully loaded image 4:\n",
      "- Shape: torch.Size([3, 300, 300])\n",
      "- Label: nasi_goreng\n",
      "- Path: IF25-4041-dataset/train\\0693.jpg\n",
      "--------------------------------------------------\n",
      "Successfully loaded image 5:\n",
      "- Shape: torch.Size([3, 300, 300])\n",
      "- Label: nasi_goreng\n",
      "- Path: IF25-4041-dataset/train\\0889.jpg\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test loading a few samples from the dataset\n",
    "print(\"Testing dataset loading...\")\n",
    "try:\n",
    "    for i in range(min(5, len(train_dataset))):\n",
    "        image, label, path = train_dataset[i]\n",
    "        print(f\"Successfully loaded image {i+1}:\")\n",
    "        print(f\"- Shape: {image.shape}\")\n",
    "        print(f\"- Label: {label}\")\n",
    "        print(f\"- Path: {path}\")\n",
    "        print(\"-\" * 50)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading sample {i}: {str(e)}\")\n",
    "    print(f\"Full error: {e.__class__.__name__}: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9945961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loaders with conservative settings...\n",
      "\n",
      "Testing batch loading...\n",
      "Successfully loaded a batch:\n",
      "- Batch image shape: torch.Size([16, 3, 300, 300])\n",
      "- Batch labels: ('bakso', 'nasi_goreng', 'gado_gado', 'nasi_goreng', 'bakso', 'soto_ayam', 'nasi_goreng', 'soto_ayam', 'gado_gado', 'bakso', 'rendang', 'rendang', 'bakso', 'soto_ayam', 'bakso', 'rendang')\n",
      "\n",
      "DataLoader configuration successful!\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders with more conservative settings\n",
    "print(\"Creating data loaders with conservative settings...\")\n",
    "\n",
    "# Start with minimal workers and no pin_memory\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Start with single-process loading\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Start with single-process loading\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Test batch loading\n",
    "print(\"\\nTesting batch loading...\")\n",
    "try:\n",
    "    # Get one batch from the train loader\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    images, labels, paths = sample_batch\n",
    "    print(f\"Successfully loaded a batch:\")\n",
    "    print(f\"- Batch image shape: {images.shape}\")\n",
    "    print(f\"- Batch labels: {labels}\")\n",
    "    print(\"\\nDataLoader configuration successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch: {str(e)}\")\n",
    "    print(f\"Full error: {e.__class__.__name__}: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2232a92c",
   "metadata": {},
   "source": [
    "## 5. Inisialisasi dan Pelatihan Model\n",
    "\n",
    "Setelah memastikan dataset berfungsi dengan baik, kita akan:\n",
    "1. Memuat model EfficientNet-B3 pre-trained\n",
    "2. Memodifikasi layer classifier\n",
    "3. Memulai pelatihan dengan early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f474f2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing EfficientNet-B3 model...\n",
      "Froze all backbone parameters\n",
      "Replaced classifier with new layers for 5 classes\n",
      "\n",
      "Training setup:\n",
      "- Device: cpu\n",
      "- Batch size: 16\n",
      "- Learning rate: 0.001\n",
      "- Number of epochs: 32\n",
      "- Image size: (300, 300)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "print(\"\\nInitializing EfficientNet-B3 model...\")\n",
    "weights = EfficientNet_B3_Weights.IMAGENET1K_V1\n",
    "model = efficientnet_b3(weights=weights)\n",
    "\n",
    "# Freeze backbone\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"Froze all backbone parameters\")\n",
    "\n",
    "# Replace classifier\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.3, inplace=True),\n",
    "    nn.Linear(1536, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(512, num_classes)\n",
    ")\n",
    "print(f\"Replaced classifier with new layers for {num_classes} classes\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "print(f\"\\nTraining setup:\")\n",
    "print(f\"- Device: {device}\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "print(f\"- Learning rate: {learning_rate}\")\n",
    "print(f\"- Number of epochs: {num_epochs}\")\n",
    "print(f\"- Image size: {img_size}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6260e959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop...\n",
      "\n",
      "Epoch: 1/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 23/55 [00:20<00:28,  1.12it/s, loss=1.34, acc=48.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted by user\n",
      "\n",
      "Final best validation accuracy: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop with error handling and early stopping\n",
    "best_val_acc = 0\n",
    "patience = 8  # Number of epochs to wait for improvement\n",
    "counter = 0    # Counter for patience\n",
    "print(\"Starting training loop...\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch: {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        try:\n",
    "            # Training\n",
    "            train_loss, train_acc = train_one_epoch(\n",
    "                model, train_loader, criterion, optimizer, device, label_to_idx\n",
    "            )\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = validate(\n",
    "                model, val_loader, criterion, device, label_to_idx\n",
    "            )\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_acc > best_val_acc:\n",
    "                counter = 0  # Reset counter\n",
    "                best_val_acc = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_val_acc': best_val_acc,\n",
    "                }, 'best_model2.pth')\n",
    "                print(f\"New best model saved! Validation accuracy: {val_acc:.2f}%\")\n",
    "            else:\n",
    "                counter += 1\n",
    "                print(f\"No improvement for {counter} epochs\")\n",
    "                if counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered! No improvement for {patience} epochs.\")\n",
    "                    break\n",
    "            \n",
    "            print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "            print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during epoch {epoch+1}: {str(e)}\")\n",
    "            print(f\"Full error: {e.__class__.__name__}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining stopped due to error: {str(e)}\")\n",
    "    print(f\"Full error: {e.__class__.__name__}: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    print(\"\\nFinal best validation accuracy:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37317523",
   "metadata": {},
   "source": [
    "## 6. Evaluasi Model\n",
    "\n",
    "Sekarang kita akan mengevaluasi performa model dengan berbagai metrik:\n",
    "\n",
    "### Metrik yang Digunakan:\n",
    "1. **Confusion Matrix**: \n",
    "   - Visualisasi prediksi vs label sebenarnya\n",
    "   - Menunjukkan kesalahan klasifikasi antar kelas\n",
    "   - Membantu identifikasi kelas yang sering tertukar\n",
    "\n",
    "2. **Classification Report**:\n",
    "   - Precision: Ketepatan prediksi positif\n",
    "   - Recall: Kemampuan mengenali sampel positif\n",
    "   - F1-score: Rata-rata harmonik precision dan recall\n",
    "   - Support: Jumlah sampel per kelas\n",
    "\n",
    "3. **Kurva ROC dan Nilai AUC**:\n",
    "   - Menunjukkan trade-off sensitivitas vs spesifisitas\n",
    "   - AUC = 1.0 berarti klasifikasi sempurna\n",
    "   - AUC = 0.5 berarti klasifikasi acak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "\n",
    "def evaluate_model(model, dataloader, device, label_to_idx, idx_to_label):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels_tuple, _ in tqdm(dataloader, desc='Evaluating'):\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Convert string labels to indices\n",
    "            if isinstance(labels_tuple[0], str):\n",
    "                label_indices = [label_to_idx[label] for label in labels_tuple]\n",
    "            else:\n",
    "                label_indices = labels_tuple\n",
    "            targets = torch.tensor(label_indices, dtype=torch.long)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# Get predictions\n",
    "print(\"Getting model predictions...\")\n",
    "val_preds, val_labels, val_probs = evaluate_model(model, val_loader, device, label_to_idx, idx_to_label)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[idx_to_label[i] for i in range(len(idx_to_label))],\n",
    "            yticklabels=[idx_to_label[i] for i in range(len(idx_to_label))])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, val_preds,\n",
    "                          target_names=[idx_to_label[i] for i in range(len(idx_to_label))]))\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])\n",
    "\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    fpr, tpr, _ = roc_curve((val_labels == i).astype(int), val_probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, lw=2,\n",
    "             label=f'ROC curve of class {idx_to_label[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Multi-class ROC Curves')\n",
    "plt.legend(loc=\"lower right\", fontsize='small')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20486a6d",
   "metadata": {},
   "source": [
    "### Analisis Hasil\n",
    "\n",
    "Mari kita pahami arti dari setiap metrik evaluasi:\n",
    "\n",
    "1. **Confusion Matrix (Matriks Kebingungan)**\n",
    "   - Elemen diagonal: Jumlah prediksi yang benar\n",
    "   - Elemen non-diagonal: Kesalahan klasifikasi\n",
    "   - Warna lebih gelap menunjukkan jumlah yang lebih besar\n",
    "   - Membantu identifikasi pola kesalahan prediksi\n",
    "\n",
    "2. **Classification Report (Laporan Klasifikasi)**\n",
    "   - Precision: Seberapa akurat prediksi positif\n",
    "   - Recall: Seberapa baik model menemukan kasus positif\n",
    "   - F1-score: Keseimbangan antara precision dan recall\n",
    "   - Support: Jumlah sampel dalam dataset\n",
    "\n",
    "3. **Kurva ROC dan AUC**\n",
    "   - True Positive Rate: Kemampuan mendeteksi kasus positif\n",
    "   - False Positive Rate: Tingkat kesalahan positif\n",
    "   - AUC mendekati 1.0: Model sangat baik\n",
    "   - AUC mendekati 0.5: Model seperti menebak acak\n",
    "\n",
    "4. **Visualisasi Prediksi**\n",
    "   - Gambar dengan label hijau: Prediksi benar\n",
    "   - Gambar dengan label merah: Prediksi salah\n",
    "   - Membantu memahami jenis kesalahan visual yang dibuat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "def show_predictions(model, dataloader, device, num_images=5):\n",
    "    model.eval()\n",
    "    all_images = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in dataloader:\n",
    "            if len(all_images) >= num_images:\n",
    "                break\n",
    "                \n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            # Convert tensors to numpy for visualization\n",
    "            images = inputs.cpu().numpy()\n",
    "            preds = preds.cpu().numpy()\n",
    "            \n",
    "            all_images.extend(images)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    # Plot images with predictions\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 4))\n",
    "    for i, (img, pred, true_label) in enumerate(zip(all_images[:num_images], \n",
    "                                                   all_preds[:num_images], \n",
    "                                                   all_labels[:num_images])):\n",
    "        # Convert from CHW to HWC format and denormalize\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        img = img * np.array(MakananIndo.IMAGENET_STD) + np.array(MakananIndo.IMAGENET_MEAN)\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        color = 'green' if idx_to_label[pred] == true_label else 'red'\n",
    "        axes[i].set_title(f'Pred: {idx_to_label[pred]}\\nTrue: {true_label}', \n",
    "                         color=color)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"Visualizing some predictions...\")\n",
    "show_predictions(model, val_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
